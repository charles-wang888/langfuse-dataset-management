# Langfuse-dataset-management 用Langfuse管理数据集


# 实验场景说明

本实验旨在解决以下场景：

我现在拥有一个数据集（如 Alpaca 或其他数据集），该数据集包含指令（instruction）、输入（input）和标准答案（output）。  
现在我已经预训练或微调了一个大模型（可以是任何大模型），需要评估其性能。具体做法是让模型完成一份题目，将每一题的作答结果与标准答案比对并计算得分（得分用于衡量 LLM 给出的答案与标准答案的差异）。

## 刚性要求

1. **大模型可替换**：可以测试不同的大模型。
2. **题目与答案可见**：每道题目的分数需结合题目和答案展示，即在同一行同时看到标准答案、LLM 答案以及评分。
3. **自定义评分函数**：计算答案相似度得分的函数可自定义。例如可以用余弦相似度，也可以基于 TF-IDF 向量化后计算差异。该函数只需接收两个文本输入，并返回一个浮点数值作为结果。
4. **实验可复用**：实验可反复进行。例如今天做了一组实验，得到一组结果，之后微调大模型后可继续跑相同用例并对比分数。
5. **题目细节可查**：可选择任意一道题目，查看 LLM 作答的具体细节。

---

# 实验描述 —— 用 Langfuse 管理数据集

LangFuse 提供了强大的数据集管理能力，能够帮助管理自定义数据集，并协助满足上述所有需求。LangFuse 支持两种管理数据集的方式：

1. **界面维护**：直接在界面上定义数据集并维护数据。
2. **本地上传**：从本地上传一个数据集到 LangFuse（推荐）。

## 创建数据集容器

1. 登录 Langfuse，进入已创建的应用（如 Charles-Demo1）。
2. 选择左侧菜单的 DataSet -> New dataset，创建一个数据集并命名。
3. 这是一个待装入数据的数据集容器，可通过程序加入数据。  
   例如选择 github 上的 Alpaca 数据集（也可选其他数据集），该数据集有 1000 条数据。  
   [Alpaca 数据集地址](https://github.com/hiyouga/LLaMA-Factory/blob/main/data/alpaca_zh_demo.json)

---

# 使用 LangFuse 数据集做大模型能力评估实验

现在 LangFuse 已经管理好数据集，接下来进行大模型实验，即让大模型针对测试集作答，然后对比大模型答案与标准答案的差距。

## 实验分解为以下子任务：

### 子任务1：自定义打分函数

- 实现一个打分函数，输入为两个文本，输出为相似度分数（如余弦相似度、TF-IDF 差异等）。

### 子任务2：定义 LLM 应用

- 通过 Langchain 的 chain 实现 LLM 应用，包含三部分：
  - 提示词（基于 PromptTemplate 构建）
  - LLM 大模型
  - 输出解析器

### 子任务3：评估器

- 在指定数据集上运行 LLM 应用并打分。
- 打分结果通过 LangChain 的回调接口传递给 LangFuse，便于统一管理和分析。

---
